# Case Study: Whose Perspective Does AI Reflect? 
### Auditing Gendered & Geographical Bias in LLMs vs. Real-World Data

## ğŸ¯ Project Objective
The primary goal of this research is to investigate whether Large Language Models (LLMs) reproduce or amplify gendered stereotypes and geographical biases in the context of urban safety. Specifically, the project aims to:
* [cite_start]Determine if LLMs provide different safety advice based on the gendered persona of the user.
* [cite_start]Evaluate the accuracy of AI-generated safety perceptions by comparing them to ground-truth crime statistics.
* [cite_start]Assess the alignment between AI "intuition" and lived human experiences captured in crowdsourced reviews.

## ğŸ“– Overview
As AI becomes a primary source for travel suggestions and neighborhood evaluations, it risks reflecting the biases present in its training data. [cite_start]This study conducts a forensic audit of **GPT-5** and **Claude Sonnet 4**, comparing their perceptions of London neighborhoods against a decade of human reviews and official police data[cite: 2]. [cite_start]The project identifies a significant "perception gap" where AI models frequently overestimate risk and introduce gendered disparities that do not exist in the real world.

## ğŸ“Š Datasets
The analysis is built upon three distinct data pillars:
* [cite_start]**Human Experience:** 1.6M+ London Airbnb reviews spanning from 2009 to 2025[cite: 2].
* [cite_start]**Ground Truth:** 969,175 official crime records from the Metropolitan Police (May 2024 â€“ May 2025)[cite: 2].
* [cite_start]**Synthetic AI Data:** 630 targeted prompts generated by GPT-5 and Claude Sonnet 4 using male, female, and neutral traveler personas[cite: 2].

## ğŸ› ï¸ Methods
The project employed a multi-stage data science pipeline:
* [cite_start]**Preprocessing:** Cleaned and normalized geographical data to the **ONS ward-level** for high-resolution mapping[cite: 2].
* [cite_start]**Gender Classification:** Utilized **Gender-Guesser** and the **Genderize.io API** to categorize human reviewers with 98% accuracy[cite: 2].
* [cite_start]**Thematic Modeling:** Implemented **BERTopic** to extract latent themes (Safety, Transportation, Amenities) from text data[cite: 2].
* [cite_start]**Fairness Auditing:** Applied statistical frameworks including **Demographic Parity** and **Equalized Odds** to measure bias across personas[cite: 2].



## ğŸ” Analysis
The analysis focused on "Semantic Divergence"â€”the gap between how AI speaks and how humans speak.
* [cite_start]**Similarity Testing:** Using Sentence Transformers, the study found that LLMs have a low semantic similarity to human reviews (GPT-5: 0.28, Claude: 0.23), suggesting AI relies on different linguistic patterns than residents[cite: 2].
* [cite_start]**Correlation Analysis:** Spearmanâ€™s Rank Correlation was used to test if AI safety ratings move in tandem with actual crime rates[cite: 2]. 
* [cite_start]**Bias Testing:** T-tests were conducted to confirm if the safety score differences between male and female prompts were statistically significant[cite: 2].

## ğŸ† Results
* [cite_start]**Safety Under-representation:** While human reviews describe 90% of neighborhoods as safe, LLMs are far more cautious, with Claude labeling only 52% and GPT-5 only 44% as safe[cite: 2].
* [cite_start]**Systematic Gender Bias:** Both models showed significant gender bias (Claude: $p=0.001$, GPT-5: $p=0.024$)[cite: 2]. [cite_start]LLMs labeled areas as "safe" for women more frequently than for men, a "protective" bias absent in Airbnb data.
* [cite_start]**Ground-Truth Mismatch:** Human reviews successfully tracked crime patterns, but LLM outputs showed weak or no correlation with official crime data[cite: 2].



## ğŸ’¡ Impact
This research provides a critical framework for **Responsible AI**. It demonstrates that:
* [cite_start]LLM-based recommendation engines can distort community perceptions and lead to economic disinvestment in unfairly labeled areas[cite: 1].
* [cite_start]Fairness metrics must be integrated into geographical and travel-based AI applications to prevent the reinforcement of social stereotypes[cite: 2].
* [cite_start]There is an urgent need for **Retrieval-Augmented Generation (RAG)** to ground AI safety advice in real-time facts rather than static, biased training data[cite: 2].

## ğŸ Conclusion
[cite_start]The study concludes that current LLMs exhibit "geographical hallucinations" and "gendered protectionism". [cite_start]While human crowdsourced data remains a reliable proxy for real-world conditions, AI models still struggle to align their safety assessments with ground truth. [cite_start]This work serves as a call to action for AI developers to prioritize transparency and bias mitigation in social-facing applications.

---
*Developed by Kostanca Kovaci | [cite_start]MSc Data Science, Middlesex University London* [cite: 2]
